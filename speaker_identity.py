# speaker_identity.py (ç¹žéŽ torchaudio è®€å–ç‰ˆ)

import torch
import torchaudio
import os
import soundfile as sf  # ðŸ‘ˆ ç›´æŽ¥ä½¿ç”¨ soundfile è®€å–ï¼Œä¸é€éŽ torchaudio

# =========================================================
# ðŸš‘ ç†±ä¿®å¾© 1ï¼šè§£æ±º speechbrain ä¾è³´å•é¡Œ
if not hasattr(torchaudio, "list_audio_backends"):
    torchaudio.list_audio_backends = lambda: ["soundfile"]
# =========================================================

from speechbrain.inference.speaker import EncoderClassifier

# ðŸ’¾ è¨­å®šä½ çš„è²éŸ³æ¨£æœ¬è·¯å¾‘
MASTER_VOICE_FILE = "master_voice.wav" 

print("â³ [è²ç´‹] æ­£åœ¨è¼‰å…¥ SpeechBrain æ¨¡åž‹...")
try:
    classifier = EncoderClassifier.from_hparams(
        source="speechbrain/spkrec-ecapa-voxceleb",
        savedir="pretrained_models/spkrec-ecapa-voxceleb",
        run_opts={"device": "cuda" if torch.cuda.is_available() else "cpu"}
    )
except Exception as e:
    print(f"âŒ [è²ç´‹] æ¨¡åž‹è¼‰å…¥å¤±æ•—: {e}")
    classifier = None

def get_embedding(wav_path):
    """å°‡è²éŸ³æª”æ¡ˆè½‰æˆè²ç´‹å‘é‡"""
    if not os.path.exists(wav_path):
        print(f"âš ï¸ [è²ç´‹] æ‰¾ä¸åˆ°æª”æ¡ˆ: {wav_path}")
        return None
        
    if classifier is None:
        return None

    try:
        # ðŸš€ æ ¸å½ˆç´šä¿®å¾©ï¼šå®Œå…¨ç¹žéŽ torchaudio.load
        # 1. ä½¿ç”¨ soundfile ç›´æŽ¥è®€å– (å®ƒå›žå‚³ numpy array)
        audio_array, sample_rate = sf.read(wav_path)
        
        # 2. è½‰æˆ PyTorch Tensor
        signal = torch.from_numpy(audio_array).float()
        
        # 3. è™•ç†ç¶­åº¦ (Soundfile æ˜¯ [æ™‚é–“, è²é“], PyTorch éœ€è¦ [è²é“, æ™‚é–“])
        if len(signal.shape) == 1:
            # å–®è²é“: [T] -> [1, T]
            signal = signal.unsqueeze(0)
        else:
            # å¤šè²é“: [T, C] -> [C, T] -> å–å¹³å‡è®Šå–®è²é“ [1, T]
            signal = signal.transpose(0, 1)
            signal = signal.mean(dim=0, keepdim=True)
            
        # è¨ˆç®—è²ç´‹
        with torch.no_grad():
            embeddings = classifier.encode_batch(signal)
            
        return embeddings
        
    except Exception as e:
        print(f"âŒ [è²ç´‹] åˆ†æžéŸ³è¨Šå¤±æ•—: {e}")
        # å°å‡ºæ›´è©³ç´°éŒ¯èª¤ä»¥ä¾¿é™¤éŒ¯
        import traceback
        traceback.print_exc()
        return None

# å¿«å–çš„è²ç´‹
master_embedding = None

def load_master_voice():
    """ç¨‹å¼å•Ÿå‹•æ™‚ï¼Œå…ˆè¨˜ä½çš„è²éŸ³"""
    global master_embedding
    if os.path.exists(MASTER_VOICE_FILE):
        print(f"âœ… [è²ç´‹] è®€å–è²éŸ³æ¨£æœ¬: {MASTER_VOICE_FILE}")
        master_embedding = get_embedding(MASTER_VOICE_FILE)
        
        if master_embedding is not None:
            print(f"âœ… [è²ç´‹] è²ç´‹è¨»å†ŠæˆåŠŸï¼")
        else:
            print(f"âŒ [è²ç´‹] è²ç´‹è®€å–å¤±æ•—ï¼Œè«‹æª¢æŸ¥ wav æª”æ¡ˆæ ¼å¼ã€‚")
            
    else:
        print(f"âš ï¸ [è²ç´‹] æ‰¾ä¸åˆ°æ¨£æœ¬ ({MASTER_VOICE_FILE})")

def identify_speaker(current_audio_path, threshold=0.45):
    """æ¯”å°ç•¶å‰çš„éŒ„éŸ³"""
    if master_embedding is None or classifier is None:
        return False, 0.0 

    current_emb = get_embedding(current_audio_path)
    if current_emb is None:
        return False, 0.0

    # è¨ˆç®—é¤˜å¼¦ç›¸ä¼¼åº¦
    score = torch.nn.functional.cosine_similarity(master_embedding, current_emb, dim=-1)
    score_val = score.mean().item()
    
    # print(f"ðŸ” [è²ç´‹] ç›¸ä¼¼åº¦å¾—åˆ†: {score_val:.4f}") 
    
    if score_val > threshold:
        return True, score_val
    else:
        return False, score_val

# å•Ÿå‹•æ™‚è‡ªå‹•è¼‰å…¥
load_master_voice()