# huggingface_r1.py â€” ä½¿ç”¨ HuggingFace DeepSeek-R1-8B + æµå¼è¼¸å‡º
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer, BitsAndBytesConfig, TextIteratorStreamer
from threading import Thread

# ğŸŒŸ ä½ å¯æ›å…¶ä»–æ¨¡å‹ï¼ˆ7B/8B/14B/32Bï¼‰
#HF_MODEL_NAME = "deepseek-ai/DeepSeek-R1"
HF_MODEL_NAME = r"D:\ai_vtuber\DeepSeek-R1-8B"

# å…¨åŸŸæ¨¡å‹å¿«å–ï¼šåªè¼‰å…¥ä¸€æ¬¡
_tokenizer = None
_model = None


def load_hf_model():
    """
    è¼‰å…¥ DeepSeek R1 æ¨¡å‹ï¼ˆåªè¼‰å…¥ä¸€æ¬¡ï¼‰
    """
    global _tokenizer, _model

    if _tokenizer is not None and _model is not None:
        return _tokenizer, _model

    print("ğŸ§  [HF] æ­£åœ¨è¼‰å…¥ DeepSeek-R1 æ¨¡å‹ï¼ˆåˆæ¬¡è¼‰å…¥æœƒèŠ±æ™‚é–“ï¼‰...")

    _tokenizer = AutoTokenizer.from_pretrained(HF_MODEL_NAME)

    if _tokenizer.pad_token is None:
        _tokenizer.pad_token = _tokenizer.eos_token

    bnb_config = BitsAndBytesConfig(
    load_in_8bit=True,
    )

    _model = AutoModelForCausalLM.from_pretrained(
        HF_MODEL_NAME,
        dtype=torch.bfloat16,            
        quantization_config=bnb_config, 
        device_map="auto",               
        trust_remote_code=True,
    )

    print("âœ… [HF] æ¨¡å‹è¼‰å…¥å®Œæˆï¼")
    return _tokenizer, _model


def get_ollama_response_stream(prompt: str, model_name: str = None):
    """
    å°‡ HuggingFace Streaming æ”¹æˆæ­£ç¢ºçš„ Thread + TextIteratorStreamer å¯«æ³•ã€‚
    """

    tokenizer, model = load_hf_model()

    # ğŸŒŸ ä¿®æ­£ 4ï¼šä½¿ç”¨ TextIteratorStreamerï¼Œå®ƒå°ˆé–€ç‚º Python Generator è¨­è¨ˆ
    streamer = TextIteratorStreamer(
        tokenizer, 
        skip_prompt=True,             # è·³é Prompt æœ¬èº«
        skip_special_tokens=True      # è·³é <|end of sentence|> ç­‰ç‰¹æ®Šæ¨™è¨˜
    )

    # è™•ç†è¼¸å…¥ Prompt
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    print("--- [DEBUG] æˆåŠŸå»ºç«‹è¼¸å…¥å¼µé‡ä¸¦æº–å‚™ç”Ÿæˆ ---")

    # ğŸ’¥ ä¿®æ­£ 5ï¼šåœ¨å–®ç¨çš„åŸ·è¡Œç·’ä¸­é‹è¡Œ model.generate() 
    # è®“ä¸»ç¨‹å¼å¯ä»¥åŒæ™‚æ¥æ”¶ streamer çš„è¼¸å‡º
    generation_kwargs = dict(
        **inputs,
        max_new_tokens=768,
        do_sample=True,
        temperature=0.8,
        top_p=0.95,
        streamer=streamer, # å°‡ streamer å‚³å…¥
    )

    # å•Ÿå‹•ç”ŸæˆåŸ·è¡Œç·’
    thread = Thread(target=model.generate, kwargs=generation_kwargs)
    thread.start()

    # ğŸŒŸ ä¿®æ­£ 6ï¼šä¸»ç¨‹å¼ yield streamer.on_text_stream 
    # é€™æ˜¯ TextIteratorStreamer å°ˆé–€è¨­è¨ˆçš„è¿­ä»£å™¨
    for new_text in streamer:
        yield new_text

    # ç­‰å¾…ç”ŸæˆçµæŸ
    thread.join()
