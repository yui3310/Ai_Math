# vllm_module.py (å°ˆç”¨æ–¼ vLLM Qwen2.5-3B)

from openai import OpenAI
from typing import Generator
import os

# --- ğŸ”§ è¨­å®šå€ ---
# vLLM ä¼ºæœå™¨åœ°å€ (WSL2 çš„ localhost é€šå¸¸å¯ä»¥äº’é€š)
VLLM_API_URL = "http://localhost:8000/v1"

# æ¨¡å‹åç¨± (å¿…é ˆè·Ÿ WSL å•Ÿå‹•æŒ‡ä»¤çš„ä¸€æ¨¡ä¸€æ¨£)
MODEL_NAME = "Qwen/Qwen3-4B-AWQ"

# åˆå§‹åŒ– OpenAI å®¢æˆ¶ç«¯ (vLLM ç›¸å®¹ OpenAI API)
client = OpenAI(
    base_url=VLLM_API_URL,
    api_key="EMPTY" # æœ¬åœ°ç«¯ä¸éœ€è¦ Key
)

def get_llm_response_stream(prompt: str) -> Generator[str, None, None]:
    """
    ç™¼é€ Prompt çµ¦ vLLM ä¸¦æµå¼æ¥æ”¶å›è¦†
    """
    try:
        print(f"ğŸš€ [vLLM] ç™¼é€è«‹æ±‚çµ¦ Qwen 3B...")
        
        # ç™¼é€èŠå¤©è«‹æ±‚
        response = client.chat.completions.create(
            model=MODEL_NAME,
            messages=[
                # vLLM æ”¯æ´æ¨™æº– Chat æ ¼å¼ï¼Œé€™è£¡ç›´æ¥æŠŠæ•´åŒ… prompt å¡çµ¦ user
                # å› ç‚ºæ‚¨çš„ main_app å·²ç¶“æŠŠ System Prompt çµ„åˆé€²å»äº†
                {"role": "user", "content": prompt}
            ],
            stream=True,
            
            # --- ğŸ­ åƒæ•¸èª¿æ ¡ (é‡å° Qwen 3B å„ªåŒ–) ---
            temperature=0.85, # å‰µæ„åº¦ (0.7~0.9)
            top_p=0.95,       # å¤šæ¨£æ€§
            max_tokens=512,   # é™åˆ¶å›ç­”é•·åº¦ (é¿å…é•·ç¯‡å¤§è«–)
            frequency_penalty=0.1, # æ¸›å°‘é‡è¤‡
            presence_penalty=0.1
        )

        # æµå¼æ¥æ”¶
        for chunk in response:
            if chunk.choices[0].delta.content:
                content = chunk.choices[0].delta.content
                yield content

    except Exception as e:
        error_msg = f"âŒ [vLLM] é€£ç·šå¤±æ•—: {e}"
        print(error_msg)
        yield error_msg

# æ¸¬è©¦å€å¡Š
if __name__ == "__main__":
    print("æ­£åœ¨æ¸¬è©¦ vLLM é€£ç·š...")
    for text in get_llm_response_stream("ä½ å¥½ï¼Œè«‹è‡ªæˆ‘ä»‹ç´¹ã€‚"):
        print(text, end="", flush=True)